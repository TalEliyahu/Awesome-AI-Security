# Awesome AI Security ğŸ›¡ï¸  

A curated repository of resources, research, tools, and discussions on AI security. This repository consolidates insights from industry reports, academic research, and practical applications.  

---

## Table of Contents  

- [Best Practices & Security Standards](#best-practices--security-standards)  
- [AI Risk & Incident Databases](#ai-risk--incident-databases)  
- [Glossary](#glossary)  
- [Taxonomy](#taxonomy)  
- [Benchmarks](#benchmarks)  
- [Datasets](#datasets)  
- [Free AI Security Courses](#free-ai-security-courses)  
- [Additional AI Security Resources](#additional-ai-security-resources)  
- [GitHub Repositories](#github-repositories)  
- [Newsletter](#newsletter)  
- [Conferences & Events](#conferences--events)  
- [Reports & Research](#reports--research)  
- [CTF Challenges](#ctf-challenges)  

---

## Best Practices & Security Standards  

- **[NIST AI Risk Management Framework (AI RMF)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf)**  
- **[NIST AI Risk Management Framework Playbook](https://airc.nist.gov/airmf-resources/playbook/)**  
- **[MITRE ATLAS](http://atlas.mitre.org)**  
- **[OWASP AI Exchange](https://owaspai.org/docs/ai_security_overview/)**  
- **[OWASP Top 10 for LLM and Generative AI](https://owasp.org/www-project-top-10-for-large-language-model-applications/)**  

---

## AI Risk & Incident Databases  

- **[MIT AI Risk Repository](https://airisk.mit.edu/)** â€“ A comprehensive database of over 1,000 AI risks, categorized by cause and domain, providing researchers, policymakers, and developers with a structured understanding of AI threats and mitigation strategies.  
- **[AI Incident Database (AIID)](https://incidentdatabase.ai/)** â€“ A public repository cataloging real-world AI failures, risks, and harms, helping researchers, policymakers, and organizations analyze patterns and enhance AI governance.  

---

## Glossary  

- **[NIST - "The Language of Trustworthy AI: An In-Depth Glossary of Terms"](https://airc.nist.gov/glossary/)**  

---

## Taxonomy  

- **[ARC PI Taxonomy](https://github.com/Arcanum-Sec/arc_pi_taxonomy)** â€“ A taxonomy of prompt injection attacks  

---

## Benchmarks  

ğŸ“Œ *(To be added â€“ A curated list of AI security evaluation benchmarks.)*  

---

## Datasets  

ğŸ“Œ *(To be added â€“ Public datasets useful for AI security research and adversarial ML testing.)*  

---

## Free AI Security Courses  

ğŸ“Œ *(To be added â€“ A curated list of free AI security courses.)*  

---

## Additional AI Security Resources  

- **[LLM Security](http://llmsecurity.net)**  
- **[Vinija AI](http://vinija.ai/models/LLM/)**  

---

## GitHub Repositories  

ğŸ“Œ *(More AI security-focused repositories to be added.)*  

---

## Newsletter  

- **[Adversarial AI Digest](https://www.linkedin.com/newsletters/adversarial-ai-digest-7298813894498598912/)**  

---

## Conferences & Events  

- **[AI Village (DEF CON)](https://aivillage.org/)**  
- **[Black Hat AI Summit](https://www.blackhat.com/)**  

ğŸ“Œ *(More to be added â€“ List of key conferences, workshops, and symposiums focused on AI security.)*  

---

## Reports & Research  

ğŸ“Œ *(More to be added â€“ A collection of AI security reports, white papers, and academic studies.)*  

---

## CTF Challenges  

- **[AI GOAT](https://github.com/dhammon/ai-goat)** â€“ Learn AI security through a series of vulnerable LLM CTF challenges. No sign-ups, no cloud fees, run everything locally on your system.  
- **[Gandalf CTF](https://gandalf.lakera.ai/)** â€“ A challenge to break an AI-powered security system by bypassing an LLM-based filter.  

ğŸ“Œ *(More CTFs to be added â€“ Feel free to suggest AI security CTFs that should be included!)*  

---

ğŸ›¡ï¸ *Contributions are welcome! Feel free to submit PRs with new resources and updates.*  
