# Awesome AI Security 🛡️  

> Curated resources, research, and tools for securing AI systems.  

[![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

---

## Table of Contents  

- [Best Practices and Security Standards](#best-practices-and-security-standards)  
- [AI Risk and Incident Databases](#ai-risk-and-incident-databases)  
- [Glossary](#glossary)  
- [Taxonomy](#taxonomy)  
- [AI Security Knowledge Bases](#ai-security-knowledge-bases)  
- [Benchmarks](#benchmarks)  
- [Datasets](#datasets)  
- [AI Security Courses](#ai-security-courses)  
- [AI Security Guides & Learning Resources](#ai-security-guides--learning-resources)  
- [Additional AI Security Resources](#additional-ai-security-resources)  
- [GitHub Repositories](#github-repositories)  
- [Newsletter](#newsletter)  
- [Conferences and Events](#conferences-and-events)  
- [Reports and Research](#reports-and-research)  
- [CTF Challenges](#ctf-challenges)  
- [Tools](#tools)  

---

## Best Practices and Security Standards  

- [NIST AI Risk Management Framework (AI RMF)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf)  
- [NIST AI Risk Management Framework Playbook](https://airc.nist.gov/airmf-resources/playbook/)  
- [OWASP AI Exchange](https://owaspai.org/docs/ai_security_overview/)  
- [OWASP Top 10 for LLM and Generative AI](https://owasp.org/www-project-top-10-for-large-language-model-applications/)  

---

## AI Risk and Incident Databases  

- [MIT AI Risk Repository](https://airisk.mit.edu/)  
- [AI Incident Database (AIID)](https://incidentdatabase.ai/)  
- [AI/ML Supply Chain Vulnerability Database](https://sightline.protectai.com/vulnerabilities)  

---

## Glossary  

- [NIST - "The Language of Trustworthy AI: An In-Depth Glossary of Terms"](https://airc.nist.gov/glossary/)  

---

## Taxonomy  

- [ARC PI Taxonomy](https://github.com/Arcanum-Sec/arc_pi_taxonomy)  

---

## AI Security Knowledge Bases  

- [MITRE ATLAS](http://atlas.mitre.org) – A knowledge base of tactics, techniques, and case studies for adversarial threats targeting AI systems.  
- [GenAI Attacks Matrix](https://ttps.ai/matrix.html#genai-attacks-matrix) – A structured knowledge base documenting TTPs used to target GenAI-based systems, copilots, and agents.  

---

## Benchmarks  

📌 *(To be added – A curated list of AI security evaluation benchmarks.)*  

---

## Datasets  

📌 *(To be added – Public datasets useful for AI security research and adversarial ML testing.)*  

---

## AI Security Courses  

- [Microsoft AI Security Learning Path](https://learn.microsoft.com/en-us/training/browse/?filter-roles=ai%20&roles=ai-engineer&subjects=security) – Free training modules on AI security, covering secure AI model development, risk management, and threat mitigation.  
- [AWS AI Security Training](https://explore.skillbuilder.aws/learn/external-ecommerce;view=none;redirectURL=?ctldoc-catalog-0=se-%22AI%20Security%22) – Free AWS courses on securing AI applications, risk management, and implementing security best practices in AI/ML environments.  

---

## AI Security Guides & Learning Resources  

- [Nightfall AI Security 101](https://www.nightfall.ai/ai-security-101) – A centralized learning hub for AI security, offering an evolving library of concepts, emerging risks, and foundational principles in securing AI systems.  

---

## Additional AI Security Resources  

- [LLM Security](http://llmsecurity.net)  
- [Vinija AI](http://vinija.ai/models/LLM/)  

---

## GitHub Repositories  

📌 *(More AI security-focused repositories to be added.)*  

---

## Newsletter  

- [Adversarial AI Digest](https://www.linkedin.com/newsletters/adversarial-ai-digest-7298813894498598912/)  

---

## Conferences and Events  

- [AI Village (DEF CON)](https://aivillage.org/)  
- [Black Hat AI Summit](https://www.blackhat.com/)  

---

## Reports and Research  

📌 *(More to be added – A collection of AI security reports, white papers, and academic studies.)*  

---

## CTF Challenges  

- [AI GOAT](https://github.com/dhammon/ai-goat) – A set of LLM security challenges focused on identifying and exploiting vulnerabilities in AI systems.  
- [Gandalf CTF](https://gandalf.lakera.ai/) – A challenge where participants attempt to extract hidden passphrases from an evolving AI model through prompt engineering techniques.  
- [DamnVulnerableLLMApplication-Demo](https://github.com/greshake/DamnVulnerableLLMApp) – A project designed for security researchers to practice LLM hacking techniques and for AI companies to improve the security of their models and systems.  

---

## Tools  

- [Agentic Security](https://github.com/msoedov/agentic_security) – An open-source vulnerability scanner for Agent Workflows and Large Language Models (LLMs). Helps protect AI systems from jailbreaks, fuzzing, and multimodal attacks.  

---

## Contributing  

Contributions are welcome! If you have new resources, tools, or insights to add, feel free to submit a pull request.  

This repository follows the **[Awesome Manifesto](https://github.com/sindresorhus/awesome/blob/main/awesome.md)** guidelines.  

---

## License  

[![CC0](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)  

This list is released under the **Creative Commons Zero v1.0 Universal** license.  
