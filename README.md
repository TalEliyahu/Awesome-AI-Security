# Awesome AI Security 🛡️  

> Curated resources, research, and tools for securing AI systems. [![Awesome](https://awesome.re/badge.svg)](https://awesome.re)

---

## Table of Contents  

- [Best Practices & Security Standards](#best-practices--security-standards)  
- [AI Risk & Incident Databases](#ai-risk--incident-databases)  
- [Glossary](#glossary)  
- [Taxonomy](#taxonomy)  
- [AI Security Knowledge Bases](#ai-security-knowledge-bases)  
- [Benchmarks](#benchmarks)  
- [Datasets](#datasets)  
- [Free AI Security Courses](#free-ai-security-courses)  
- [Additional AI Security Resources](#additional-ai-security-resources)  
- [GitHub Repositories](#github-repositories)  
- [Newsletter](#newsletter)  
- [Conferences and Events](#conferences-and-events)  
- [Reports and Research](#reports-and-research)  
- [CTF Challenges](#ctf-challenges)  
- [Tools](#tools)  

---

## [↑](#table-of-contents) AI Security Knowledge Bases  

- [MITRE ATLAS](http://atlas.mitre.org) –  A knowledge base of tactics, techniques, and case studies for adversarial threats targeting AI systems.
- [GenAI Attacks Matrix](https://ttps.ai/matrix.html#genai-attacks-matrix) – A structured knowledge base documenting TTPs used to target GenAI-based systems, copilots, and agents.  

---

## [↑](#table-of-contents) Best Practices & Security Standards  

- [NIST AI Risk Management Framework (AI RMF)](https://nvlpubs.nist.gov/nistpubs/ai/NIST.AI.100-1.pdf)  
- [NIST AI Risk Management Framework Playbook](https://airc.nist.gov/airmf-resources/playbook/)  
- [OWASP AI Exchange](https://owaspai.org/docs/ai_security_overview/)  
- [OWASP Top 10 for LLM and Generative AI](https://owasp.org/www-project-top-10-for-large-language-model-applications/)  

---

## [↑](#table-of-contents) AI Risk & Incident Databases  

- [MIT AI Risk Repository](https://airisk.mit.edu/)  
- [AI Incident Database (AIID)](https://incidentdatabase.ai/)  
- [AI/ML Supply Chain Vulnerability Database](https://sightline.protectai.com/vulnerabilities) – A database tracking vulnerabilities in AI/ML supply chains, providing insights into threats affecting AI models and infrastructure.  

---

## [↑](#table-of-contents) Glossary  

- [NIST - "The Language of Trustworthy AI: An In-Depth Glossary of Terms"](https://airc.nist.gov/glossary/)  

---

## [↑](#table-of-contents) Taxonomy  

- [ARC PI Taxonomy](https://github.com/Arcanum-Sec/arc_pi_taxonomy) – A taxonomy of prompt injection attacks.  

---

## [↑](#table-of-contents) Benchmarks  

📌 *(To be added – A curated list of AI security evaluation benchmarks.)*  

---

## [↑](#table-of-contents) Datasets  

📌 *(To be added – Public datasets useful for AI security research and adversarial ML testing.)*  

---

## [↑](#table-of-contents) Free AI Security Courses  

- [Microsoft AI Security Learning Path](https://learn.microsoft.com/en-us/training/browse/?filter-roles=ai%20&roles=ai-engineer&subjects=security) – Free training modules on AI security, covering secure AI model development, risk management, and threat mitigation.
- [AWS AI Security Training](https://explore.skillbuilder.aws/learn/external-ecommerce;view=none;redirectURL=?ctldoc-catalog-0=se-%22AI%20Security%22) – Free AWS courses on securing AI applications, risk management, and implementing security best practices in AI/ML environments.  


📌 *(To be added – A curated list of free AI security courses.)*  

---

## [↑](#table-of-contents) Additional AI Security Resources  

- [LLM Security](http://llmsecurity.net)  
- [Vinija AI](http://vinija.ai/models/LLM/)  

---

## [↑](#table-of-contents) GitHub Repositories  

📌 *(More AI security-focused repositories to be added.)*  

---

## [↑](#table-of-contents) Newsletter  

- [Adversarial AI Digest](https://www.linkedin.com/newsletters/adversarial-ai-digest-7298813894498598912/)  

---

## [↑](#table-of-contents) Conferences & Events  

- [AI Village (DEF CON)](https://aivillage.org/)  
- [Black Hat AI Summit](https://www.blackhat.com/)  

---

## [↑](#table-of-contents) Reports & Research  

📌 *(More to be added – A collection of AI security reports, white papers, and academic studies.)*  

---

## [↑](#table-of-contents) CTF Challenges  

- [AI GOAT](https://github.com/dhammon/ai-goat) – A set of LLM security challenges focused on identifying and exploiting vulnerabilities in AI systems.  
- [Gandalf CTF](https://gandalf.lakera.ai/) – A challenge where participants attempt to extract hidden passphrases from an evolving AI model through prompt engineering techniques.  
- [DamnVulnerableLLMApplication-Demo](https://github.com/greshake/DamnVulnerableLLMApp) – A project designed for security researchers to practice LLM hacking techniques and for AI companies to improve the security of their models and systems.  

---

## [↑](#table-of-contents) Tools  

- [Agentic Security](https://github.com/msoedov/agentic_security) – An open-source vulnerability scanner for Agent Workflows and Large Language Models (LLMs). Helps protect AI systems from jailbreaks, fuzzing, and multimodal attacks.  

---

## Contributing  

Contributions are welcome! If you have new resources, tools, or insights to add, feel free to submit a pull request.  

This repository follows the **[Awesome Manifesto](https://github.com/sindresorhus/awesome/blob/main/awesome.md)** guidelines.  

---

## License  

[![CC0](https://mirrors.creativecommons.org/presskit/buttons/88x31/svg/cc-zero.svg)](https://creativecommons.org/publicdomain/zero/1.0/)  

This list is released under the **Creative Commons Zero v1.0 Universal** license.  
