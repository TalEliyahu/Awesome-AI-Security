# Research Papers

A curated list of recent research relevant to AI security.

## August 2025

- ğŸ“– **[DIRF: A Framework for Digital Identity Protection and Clone Governance in Agentic AI Systems](https://www.arxiv.org/pdf/2508.01997)** â€” Proposes a Digital Identity Rights Framework to govern biometric/behavioral likeness, detection, and lawful use of AI-generated clones.

- ğŸ“– **[LLMs in the SOC: An Empirical Study of Human-AI Collaboration in Security Operations Centres](https://arxiv.org/pdf/2508.18947)** â€” Longitudinal study (3,090 queries, 45 analysts, 10 months) finds LLMs aid sensemaking/context-building while humans retain decision authority.

- ğŸ“– **[School of Reward Hacks: Hacking harmless tasks generalizes to misaligned behavior in LLMs](https://arxiv.org/pdf/2508.17511)** â€” Trains models to â€œreward hackâ€ on low-stakes tasks and shows behaviors generalize, illuminating misalignment risks in practice.

- ğŸ“– **[PentestJudge: Judging Agent Behavior Against Operational Requirements](https://arxiv.org/pdf/2508.02921)** â€” LLM-as-judge framework to evaluate pentesting agents against operational criteria, calibrated to human expert ground truth.

- ğŸ“– **[Incident Analysis for AI Agents](https://arxiv.org/pdf/2508.14231v1)** â€” Argues for richer, privacy-aware incident reporting (e.g., prompts, tool logs) to understand and prevent harmful agent incidents.

- ğŸ“– **[On the Security and Privacy of Federated Learning: A Survey](https://www.arxiv.org/abs/2508.13730)** â€” Comprehensive review of >200 papers on FL attacks/defenses, frameworks, applications, and open directions.

- ğŸ“– **[A Guide to Stakeholder Analysis for Cybersecurity Researchers](https://arxiv.org/pdf/2508.14796)** â€” Practical ethics guide mapping stakeholder types to research methods with worked examples for security studies.

- ğŸ“– **[In-Training Defenses against Emergent Misalignment in Language Models](https://arxiv.org/pdf/2508.06249)** â€” Studies safeguards during fine-tuning (including hosted fine-tune APIs) to mitigate emergent misalignment beyond the target domain.

- ğŸ“– **[Improving Google A2A Protocol: Protecting Sensitive Data and Mitigating Unintended Harms in Multi-Agent Systems](https://arxiv.org/pdf/2505.12490v3)** â€” Identifies protocol weaknesses and proposes zero-trust-inspired enhancements for consent, auth, and data-flow transparency.

- ğŸ“– **[Searching for Privacy Risks in LLM Agents via Simulation](https://arxiv.org/pdf/2508.10880)** â€” Simulation framework to probe privacy failures when multiple agents collaborate and exchange sensitive information.

- ğŸ“– **[Autonomous Blue-Team LLM Agent for Web Attack Forensics](https://arxiv.org/pdf/2508.20643)** â€” â€œCyberSleuthâ€ agent reconstructs web intrusions from traces/logs, identifies CVEs, and generates structured forensic reports.

- ğŸ“– **[Advancing Autonomous Incident Response: Leveraging LLMs and Cyber Threat Intelligence](https://arxiv.org/pdf/2508.10677)** â€” RAG-based IR pipeline that fuses CTI retrieval with alert context to automate enrichment and decision support.

- ğŸ“– **[Securing Agentic AI: Threat Modeling and Risk Analysis for Network Monitoring Agentic AI System](https://arxiv.org/pdf/2508.10043)** â€” Applies a MAESTRO-style 7-layer model to enumerate threats/risks introduced by network-monitoring agents.

- ğŸ“– **[Ransomware 3.0: Self-Composing and LLM-Orchestrated](https://arxiv.org/pdf/2508.20444v1)** â€” Conceptualizes end-to-end LLM-driven ransomware that probes, plans, deploys payloads, and personalizes extortion autonomously.

- ğŸ“– **[Deep Ignorance: Filtering Pretraining Data Builds Tamper-Resistant Safeguards into Open-Weight LLMs](https://arxiv.org/pdf/2508.06601)** â€” Tests whether filtering unsafe pretraining data (e.g., biorisk) can reduce downstream hazardous capabilities in open models.

- ğŸ“– **[Invitation Is All You Need! Promptware Attacks Against LLM-Powered Assistants in Production Are Practical and Dangerous](https://arxiv.org/pdf/2508.12175)** â€” Demonstrates â€œpromptwareâ€ attacks that exploit integration surfaces of production assistants; releases site with examples.

- ğŸ“– **[Training Language Model Agents to Find Vulnerabilities with CTF-DOJO](https://arxiv.org/pdf/2508.18370)** â€” Introduces CTF-DOJO, an execution environment with hundreds of Dockerized CTF challenges (from pwn.college) to train and evaluate vuln-finding agents end-to-end.

---

## July 2025

- ğŸ“– **[We Urgently Need Privilege Management in MCP: A Measurement of API Usage in MCP Ecosystems](https://arxiv.org/abs/2507.06250)** â€” Measurement of 2,562 MCP servers across 23 categories shows heavy use of network/system/file APIs with over-privileged access and weak isolation, creating tampering and data-exfiltration risks.

- ğŸ“– **[TRiSM for Agentic AI: A Review of Trust, Risk, and Security Management in LLM-based Multi-Agent Systems](https://arxiv.org/abs/2506.04133)** â€” Survey of lifecycle safeguards for agentic systems (prompt infection, memory poisoning, collusion, tool misuse) aligned to NIST AI RMF and OWASP LLM Top 10.

- ğŸ“– **[AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models](https://arxiv.org/abs/2506.14682)** â€” Benchmark of 70 CTF-style challenges for offensive tasks; headline results include strong prompt-injection performance but weak system exploitation/model inversion.

- ğŸ“– **[A Survey of LLM-Driven AI Agent Communication: Protocols, Security Risks, and Defense Countermeasures](https://arxiv.org/abs/2506.19676)** â€” Reviews agent communication protocols (e.g., MCP, A2A), stages, threat surfaces, and mitigations such as sandboxing and monitoring.

- ğŸ“– **[RepoAudit: An Autonomous LLM-Agent for Repository-Level Code Auditing](https://arxiv.org/abs/2501.18160)** â€” Memory-augmented agent with validator for end-to-end repo audits; reports 78.43% precision and 185 new bugs found (most confirmed/fixed).

- ğŸ“– **[Decompiling Smart Contracts with a Large Language Model](https://arxiv.org/pdf/2506.19624)** â€” LLM-based semantic analysis of EVM bytecode to surface vulnerabilities and malicious logic amid low verification rates.

- ğŸ“– **[Dynamic Risk Assessments for Offensive Cybersecurity Agents](https://arxiv.org/pdf/2505.18384)** â€” Argues static evaluations understate risk; proposes compute-aware, continuously updated risk scoring during agent operations.

- ğŸ“– **[When LLMs Autonomously Attack](https://engineering.cmu.edu/news-events/news/2025/07/24-when-llms-autonomously-attack.html)** â€” CMU demonstrates LLMs planning/executing cyberattacks in enterprise-grade networks; implications for defense and governance.

- ğŸ“– **[ETrace: Event-Driven Vulnerability Detection in Smart Contracts via LLM-Based Trace Analysis](https://arxiv.org/pdf/2506.15790)** â€” Event-centric trace reasoning enables vuln detection without source code.

- ğŸ“– **[BaxBench: Can LLMs Generate Correct and Secure Backends?](https://arxiv.org/abs/2502.11844) ([PDF](https://arxiv.org/pdf/2502.11844))** â€” 392-task benchmark on production-grade backend generation; even â€œcorrectâ€ code often remains exploitable, with larger drops on less common frameworks.

- ğŸ“– **[Autonomous AI-based Cybersecurity Framework for Critical Infrastructure](https://arxiv.org/abs/2507.07416)** â€” Hybrid framework for real-time vuln detection, threat modeling, and automated remediation across energy/health/transport/water sectors.

- ğŸ“– **[SafeGenBench: A Benchmark Framework for Security Vulnerability Detection in LLM-Generated Code](https://arxiv.org/abs/2506.05692)** â€” 558 tasks, 44 CWEs, 13 languages; zero-shot â€œsecure accuracyâ€ ~37%, rising to ~74% with few-shot; memory-safety best, insecure configuration worst.

- ğŸ“– **[Red Teaming AI Red Teaming](https://arxiv.org/pdf/2507.05538v1)** â€” Critical examination of AI red-teaming practice, scope, and methodology; highlights gaps and standardization needs.

- ğŸ“– **[From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agent Workflows](https://arxiv.org/abs/2506.23260)** â€” Unified threat model spanning input manipulation, model compromise, privacy/system attacks, and protocol exploits (MCP/ACP/A2A).

- ğŸ“– **[Vulnerability Detection Model using LLM and Code Chunk](https://arxiv.org/pdf/2506.19453)** â€” Function-level vuln localization aims to reduce OSS supply-chain risk; discusses distinguishing true fixes from unrelated patches.

- ğŸ“– **[Trivial Trojans: How Minimal MCP Servers Enable Cross-Tool Exfiltration of Sensitive Data](https://arxiv.org/abs/2507.19880)** â€” PoC shows benign-looking â€œweatherâ€ MCP server can discover and abuse other tools to exfiltrate data via MCP trust boundaries.

- ğŸ“– **[Security Challenges in AI Agent Deployment: Insights from a Large-Scale Public Competition](https://arxiv.org/abs/2507.20526)** â€” Results from red-teaming 22 frontier agents across 44 scenarios (1.8M prompt injections, 60k+ violations); introduces the ART benchmark.

---

## June 2025

- ğŸ“– **[AIRTBench: Measuring Autonomous AI Red Teaming Capabilities in Language Models](https://arxiv.org/abs/2506.14682)** â€” Benchmark suite and metrics for assessing LLMsâ€™ autonomous red-team capabilities across realistic adversarial tasks.

- ğŸ“– **[VulBinLLM: LLM-powered Vulnerability Detection for Stripped Binaries](https://arxiv.org/abs/2505.22010)** â€” Uses LLM reasoning over disassembly and program artifacts to detect vulnerabilities even when symbols are removed.

- ğŸ“– **[CAI: An Open, Bug Bounty-Ready Cybersecurity AI](https://arxiv.org/abs/2504.06017)** â€” Open system that orchestrates LLMs for bug bounty workflows (recon, triage, exploit ideation) with reproducible evaluations.

- ğŸ“– **[Dynamic Risk Assessments for Offensive Cybersecurity Agents](https://arxiv.org/abs/2505.18384)** â€” Proposes real-time risk scoring and governance for AI agents during offensive security operations.

- ğŸ“– **[Design Patterns for Securing LLM Agents against Prompt Injections](https://arxiv.org/abs/2506.08837)** â€” Catalog of defensive patterns (isolation, allow-lists, mediation, auditing) to harden agent architectures.

- ğŸ“– **[PANDAGUARD: Systematic Evaluation of LLM Safety against Jailbreaking Attacks](https://arxiv.org/abs/2505.13862)** â€” A standardized framework to measure jailbreak robustness across models and attack families.

- ğŸ“– **[Lessons from Defending Gemini Against Indirect Prompt Injections](https://arxiv.org/abs/2505.14534)** â€” Case studies and mitigations for cross-context (RAG/tools) prompt injection discovered in the wild.

- ğŸ“– **[Enterprise-Grade Security for the Model Context Protocol (MCP): Frameworks and Mitigation Strategies](https://arxiv.org/abs/2504.08623)** â€” Security controls for MCP deployments, including sandboxing, zero-trust tool access, and per-request policies.

- ğŸ“– **[Securing AI Agents with Information-Flow Control](https://arxiv.org/abs/2505.23643)** â€” Applies IFC to constrain data movement across agent tools, memory, and environments to prevent exfiltration and misuse.

- ğŸ“– **[Common Corpus: The Largest Collection of Ethical Data for LLM Pre-Training](https://arxiv.org/abs/2506.01732)** â€” Introduces a large, license-screened open pretraining corpus designed for regulatory compliance.

- ğŸ“– **[A Novel Zero-Trust Identity Framework for Agentic AI: Decentralized Authentication and Fine-Grained Access Control](https://arxiv.org/abs/2505.19301)** â€” Identity and authorization architecture tailored to multi-agent ecosystems with least-privilege access.

- ğŸ“– **[OS-HARM: A Benchmark for Measuring Safety of Computer Use Agents](https://arxiv.org/pdf/2506.14866)** â€” Introduces a safety benchmark for GUI-operating agents, highlighting overlooked risks in â€œcomputer useâ€ systems.

- ğŸ“– **[Malicious AI Models Undermine Software Supply-Chain Security](https://cacm.acm.org/research/malicious-ai-models-undermine-software-supply-chain-security/)** â€” CACM research article on how malicious/poisoned models create software supply-chain risks and suggested mitigations.

---

## May 2025

- ğŸ“– **[Understanding LLM Supply Chains](https://arxiv.org/abs/2504.20763)** â€” A deep mapping of 15,000+ open-source packages reveals how a small number of libraries (like transformers, langchain) dominate the ecosystem. A single vulnerability in one core package can indirectly affect 1,000+ others via transitive dependencies. Most issues go unreported through formal CVEs.

- ğŸ“– **[Full-Stack Safety Survey for LLMs](https://arxiv.org/abs/2504.15585)** â€” The first end-to-end review of safety challenges across the LLM lifecycle â€” from data collection to commercialization. Synthesizes over 800 papers, highlights risks in alignment, deployment, and model editing, and proposes forward-looking safeguards.

- ğŸ“– **[The Leaderboard Illusion](https://arxiv.org/pdf/2504.20879)** â€” A multi-institutional audit of Chatbot Arena exposes manipulation risks, duplicated prompts, silent model removal, and data inequality. Open-weight models receive less training data and sampling attention, distorting perception of model quality.

- ğŸ“– **[LLM Agent Privacy & Security Survey](https://arxiv.org/html/2407.19354v1)** â€” Categorizes nine key vulnerabilities in LLM agents, from hallucinations and knowledge poisoning to data leakage and agent manipulation. Case studies reveal the real-world impacts and gaps in current defenses.

- ğŸ“– **[Control Levels for LLM Agents](https://arxiv.org/abs/2504.05259)** â€” Introduces a 5-level framework (ACLs 1â€“5) to align security measures with AI agent capabilities. Offers structured control evaluation rules and red teaming affordances for increasingly powerful agents.

- ğŸ“– **[Package Hallucination in LLMs](https://arxiv.org/abs/2406.10279)** â€” LLMs frequently invent nonexistent libraries (e.g., `pip install xyz`) â€” a risk now named *slopsquatting*. Adversaries can register these fake packages with malware. Study shows 19.7% hallucination rate across 16 LLMs and 576K samples.

- ğŸ“– **[Enterprise-Grade MCP Security](https://arxiv.org/abs/2504.08623)** â€” AWS-backed paper on securing AI toolchains using the Model Context Protocol. Recommends sandboxing, zero-trust design, and per-request access policies to mitigate tool poisoning, data leaks, and exfiltration.

- ğŸ“– **[Can LLMs Classify CVEs?](https://arxiv.org/pdf/2504.10713v1)** â€” Proposes a hybrid CVSS scoring pipeline combining Gemma 3 for objective fields with MiniLM+XGBoost for subjective ones, achieving 84% accuracy.

- ğŸ“– **[Open Problems in Technical AI Governance](https://arxiv.org/abs/2407.14981)** â€” Maps unresolved challenges in verifying, monitoring, and securing AI systems â€” from data traceability to model access policies and third-party audits.

- ğŸ“– **[RAG LLMs Are Not Safer](https://arxiv.org/pdf/2504.18041)** â€” Retrieval-augmented generation changes model behavior and risk profile. Even safe docs + safe models can yield unsafe outputs. Study shows red teaming methods are less effective in RAG settings.

