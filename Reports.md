## August 2025

- [MLOps Overview](https://www.linkedin.com/feed/update/urn:li:activity:7366954293167325186) - This comprehensive overview explores how DevSecOps practices apply to the ML lifecycle through MLOps, along with Large Language Model Operations (LLMOps), and AI Agent Operations (AgentOps). It reveals that traditional security approaches are insufficient for ML systems due to novel threats such as data poisoning, model inversion, adversarial attacks, and member inference attacks. by Cloud Security Alliance

- [Detecting and countering misuse of AI](https://www.linkedin.com/feed/update/urn:li:activity:7366579281214308353) - This represents the work of Threat Intelligence: a dedicated team at Anthropic finds deeply investigated sophisticated real world cases of misuse and works with the rest of the Safeguards organization to improve our defenses against such cases. at Anthropic

- [Agentic AI Identity and Access Management: A New Approach](https://www.linkedin.com/feed/update/urn:li:activity:7363712903067127808) - This publication from the Cloud Security Alliance (CSA) introduces a purpose-built Agentic AI IAM framework that accounts for autonomy, ephemerality, and delegation patterns of AI agents in complex Multi-Agent Systems (MAS). It provides security architects and identity professionals with a blueprint to manage agent identities using Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and Zero Trust principles, while addressing operational challenges like secure delegation, policy enforcement, and real-time monitoring.

- [OWASP AI Maturity Assessment + Excel Assessment Tool](https://www.linkedin.com/feed/update/urn:li:activity:7361898638551240705) - AIMA supports organizations in evaluating how well their AI systems align with strategic goals, ethical principles, and operational needs. The model spans five core domains: Strategy, Design, Implementation, Operations, and Governance. Each domain includes actionable maturity levels to guide adoption and improvement.

- [Design Principles for LLM-based Systems with Zero Trust](https://www.linkedin.com/feed/update/urn:li:activity:7361109040367611904) - Six principles from Federal Office for Information Security (BSI) and ANSSI - Agence nationale de la sécurité des systèmes d'information outline how to apply Zero Trust to LLM-based systems, reducing attack surfaces, detecting threats early, and ensuring safe, transparent operation.

- [Strengthening Emergency Preparedness and Response for AI Loss of Control Incidents](https://www.linkedin.com/feed/update/urn:li:activity:7361039631129227266) -  

- [AI Security Solutions Landscape - Agentic AI Q3 2025](https://www.linkedin.com/feed/update/urn:li:activity:7360665992500273152) - The Solutions Landscape monitors and maps the full Agentic AI lifecycle, focusing on the DevOps–SecOps intersection to meet evolving security needs. Guided by the Agentic AI Threats and Mitigations guide and SecOps tasks, it highlights open-source and commercial solutions by stage, identifying their coverage of Agentic SecOps duties and threat mitigation, and leverages industry and community input as a peer-reviewed resource for navigating agentic AI’s shifting security challenges. Updated Quarterly.

- [State of Agentic AI Security and Governance](https://www.linkedin.com/feed/update/urn:li:activity:7359398882205683713) - The State of Agentic AI Security and Governance provides a comprehensive view of today’s landscape for securing and governing autonomous AI systems. It explores the frameworks, governance models, and global regulatory standards shaping responsible Agentic AI adoption. Designed for developers, security professionals, and decision-makers, the report serves as a practical guide for navigating the complexities of building, managing, and deploying agentic applications safely and effectively.

- [A Practical Guide for Building Robust AI/ML Pipeline Security](https://www.linkedin.com/feed/update/urn:li:activity:7358835243413360641) - An overview of DevSecOps practices that are applicable to MLSecOps. Lessons learned from DevSecOps can proactively address security challenges in the emerging AI/ML lifecycle.

- [Smart Cities and Critical Infrastructure AI Security Framework](https://www.linkedin.com/feed/update/urn:li:activity:7358680697135403010) - The Smart Cities Critical Infrastructure (SCCI) AI Framework is designed to provide a comprehensive, sector-agnostic approach to securing and optimizing the use of artificial intelligence across urban critical infrastructure domains. As cities increasingly rely on interconnected digital systems—ranging from emergency dispatch and healthcare to law enforcement and utilities—the need for robust, adaptable, and transparent AI governance becomes paramount. This framework addresses the unique challenges posed by AI integration, including data privacy, operational resilience, regulatory compliance, and the mitigation of emerging cyber threats.

- [Secure Agentic System Design: A Trait-Based Approach](https://www.linkedin.com/feed/update/urn:li:activity:7357415085222318080) - CSAThis publication from the CSA AI Technology and Risk Working Group addresses the unique security challenges of agentic AI. As AI transitions from passive tools to autonomous decision-makers, traditional security frameworks struggle to contextualize these new risks. Instead, we need a trait-based approach to agentic system security that identifies fundamental patterns in agent behavior and their associated vulnerabilities.

- [The AI Oversight Gap - Cost of a Data Breach Report 2025](https://www.linkedin.com/feed/update/urn:li:activity:7357597342088224769) - The report combines data from 600 actual breaches with 3,470 interviews of security and business leaders. It uses activity-based costing to quantify breach impacts across detection, response, notification, and lost business.

## July 2025

- [SBOM for AI Use Cases](https://www.linkedin.com/feed/update/urn:li:activity:7352420212471709697) — This community-driven resource helps organizations apply SBOM practices to AI systems. It highlights key use cases that address business, legal, and security risks introduced by GenAI and LLMs—many of which mirror known software supply chain challenges. SBOM for AI offers a standardized way to improve transparency, trust, and governance across AI deployments, supporting stakeholders in security, compliance, and legal functions.

- [Securing Agentic Applications Guide](https://www.linkedin.com/feed/update/urn:li:activity:7355648811236511745) - This guide aims to provide practical and actionable guidance for designing, developing, and deploying secure agentic applications powered by large language models (LLMs). It complements the OWASP Agentic AI Threats and Mitigations (ASI T&M) document by focusing on concrete technical recommendations that builders and defenders can apply directly.

- [America’s AI Action Plan – 12 AI Cybersecurity Priorities](https://www.linkedin.com/feed/update/urn:li:activity:7353987917704294400) — The U.S. outlines a national strategy to secure AI, focusing on secure-by-design development, AI incident response, and threat intelligence sharing via an AI-ISAC. Frontier AI models will undergo national security risk evaluations, while deepfake detection standards and military-grade AI data centers are prioritized. The plan also targets IP protection, critical infrastructure defense, AI vulnerability sharing, red-teaming, export controls, and foreign model assessments.

- [Google's Approach for Secure AI Agents](https://www.linkedin.com/feed/update/urn:li:activity:7347701762813829120) - As part of Google's ongoing efforts to define best practices for secure AI systems, we’re sharing our aspirational framework for secure AI agents. We advocate for a hybrid, defense-in-depth strategy that combines the strengths of traditional, deterministic security controls with dynamic, reasoning-based defenses. This approach is grounded in three core principles: agents must have well-defined human controllers, their powers must be carefully limited, and their actions and planning must be observable. This paper reflects our current thinking and the direction of our efforts as we work towards ensuring that AI agents can be powerful, useful, and secure.

- [Preparing Defenders of AI Systems V1.0](https://github.com/cosai-oasis/ws2-defenders/blob/main/preparing-defenders-of-ai-systems.md) — A community-led paper hosted by the Coalition for Secure AI explores how enterprise AI adoption reshapes security priorities. As AI systems shift from models to agents, traditional frameworks fall short. The paper emphasizes layered defenses, governance gaps, and the urgent need for AI-specific security strategies.

- [AI Controls Matrix by Cloud Security Alliance](https://cloudsecurityalliance.org/artifacts/ai-controls-matrix) - The AI Controls Matrix (AICM) is a first-of-its-kind vendor-agnostic framework for cloud-based AI systems. Organizations can use the AICM to develop, implement, and operate AI technologies in a secure and responsible manner. Developed by industry experts, the AICM builds on CSA’s Cloud Controls Matrix (CCM) and incorporates the latest AI security best practices.The AICM contains 243 control objectives distributed across 18 security domains. It maps to leading standards, including ISO 42001, ISO 27001, NIST AI RMF 1.0, and BSI AIC4. The AICM is freely available to download.

- [AI Safety Practices Compared – 2025 FLI Report](https://www.linkedin.com/feed/update/urn:li:activity:7352842152759971840) — The Future of Life Institute (FLI) evaluated Anthropic, OpenAI, DeepMind, Meta, xAI, Zhipu AI, and DeepSeek across 33 safety indicators. Key gaps include limited cyber misuse testing, weak red-teaming, missing internal safeguards, absent incident reporting, and lack of bug bounties. Only a few firms disclosed prompts or ran adversarial evaluations.

- [AI Risk Trends – 2025 Team8 CISO Village](https://www.linkedin.com/feed/update/urn:li:activity:7353987917704294400) — Based on input from 110+ CISOs, the report shows 67% of enterprises use AI agents, 25% faced AI-driven attacks, and 77% expect AI to replace SOC analyst tasks. Shadow AI is a growing risk, with many organizations lacking proper tool governance. Also covers SaaS vs in-house agent development, employee usage policies, and AI's role in threat modeling and pentesting.

- [Understanding and Safeguarding Children’s Use of AI Chatbots – Internet Matters](https://www.linkedin.com/feed/update/urn:li:activity:7351249793546924045) — Highlights risks such as misinformation, harmful content, emotional dependence, and privacy issues due to weak safeguards, lack of age checks, and limited adult guidance. Emphasizes the need for age-appropriate design and better content moderation.

- [AI Coding Assistants: Security-Safe Navigation – Secure Code Warrior](https://www.linkedin.com/feed/update/urn:li:activity:7351007465494171649) — LLMs boost speed but carry serious security risks. Key findings: even top models like OpenAI o3 are only 46.9% correct and secure, code correctness doesn’t mean code safety, training data often contains insecure patterns, and CWEs like XSS and SQLi persist. LLMs lack runtime awareness, misconfigurations are common, default tools don’t enforce secure policies, and malicious models pose supply chain risks.

- [Cyber and Artificial Intelligence Risk in Financial Services](https://www.linkedin.com/feed/update/urn:li:activity:7347449832065613825)

- [The AI Tech Stack: A Primer for Tech and Cyber Policy](https://www.linkedin.com/feed/update/urn:li:activity:7348456040172048385) — Paladin Capital Group’s report defines five core layers of the AI stack: Governance (responsible deployment via security, legal, and ethical frameworks), Application (interfaces like APIs and dashboards), Infrastructure (hardware, cloud, and compute for training/inference), Models (algorithms and ML approaches), and Data (the raw material shaping model intelligence). The report emphasizes integrating security across all layers to ensure trusted, safe, and innovation-friendly AI systems.

- [AI Maturity Model for Cybersecurity – Darktrace](https://www.linkedin.com/feed/update/urn:li:activity:7352156064244514816) — A 5-level framework guiding CISOs from manual operations to autonomous defense: Manual Operations, Automation Rules, AI Assistance, AI Collaboration, and AI Delegation. It highlights the shift from manual overload to AI-driven detection, investigation, and response with human governance.

- [The SAIL (Secure AI Lifecycle) Framework - A Practical Guide for Building and Deploying Secure AI Applications](https://www.linkedin.com/feed/update/urn:li:activity:7346638798056800257/)

- [State of Cybersecurity Resilience 2025 – Accenture](https://www.linkedin.com/feed/update/urn:li:activity:7350979113358184451) — AI-driven threats are outpacing defenses, with 90% of companies lacking maturity to counter modern attacks and 77% missing foundational AI security practices. Only 10% of organizations are “Reinvention-Ready,” combining strong strategy and capabilities. Key actions recommended: build fit-for-purpose security governance, design generative AI-secure digital cores, maintain resilient AI systems, and leverage AI to automate and detect threats.

- [Databricks AI Governance Framework](https://www.linkedin.com/feed/update/urn:li:activity:7346374987928260613) - A comprehensive guide to implementing enterprise AI programs responsibly and effectively.

- [State of LLM Application Security – Cobalt](https://www.linkedin.com/feed/update/urn:li:activity:7352038190066651136) — Key findings show 32% of LLM pentest issues are high or critical, with prompt injection (11.5%) and sensitive data leaks (14.5%) as major concerns. Risks include model denial of service, excessive agency, training data leakage (37%), data poisoning (42%), and bias. Only 21% of serious AI-specific vulnerabilities are remediated, underscoring gaps in LLM security practices.

- [Multi-Layered AI Defense – Darktrace](https://www.linkedin.com/feed/update/urn:li:activity:7352381103879475203) — Darktrace outlines a transparent, multi-layered AI approach combining unsupervised, supervised, and generative AI for continuous Learn → Detect → Investigate → Respond → Re-learn cycles. Logic and thresholds are accessible and adjustable, supporting real-time defense with human oversight.

- [Trustworthiness for AI in Defence](https://eda.europa.eu/docs/default-source/brochures/taid-white-paper-final-09052025.pdf) - The purpose of this document is to collect, present and describe the aspects of Trustworthiness for AI in Defence in a ‘food for thought’ approach reflecting the combined view of AI experts and stakeholders from Defence Industry, Academia and Ministries of Defence. This effort is performed in the context of the European Defence Agency’s (EDA) Action Plan on Artificial Intelligence for Defence and tries to address the topics of trusted AI and verification, validation and certification requirements analysis. The topics covered and analysed in this document will provide the appropriate knowledge of the current global status considering the AI regulations, standards and frameworks for AI trustworthiness and will also recommend the follow-up activities that will further assist the EU Members States and Defence Industry to better prepare, plan and develop the future AI systems aligned with the identified expectations.

- [The Mitigating ‘Hidden’ AI Risks Toolkit](https://www.linkedin.com/feed/update/urn:li:activity:7349585719247446016) — A practical guide from UK Government Communications for identifying and managing unintended AI risks. Built on lessons from deploying Assist, the first cross-government GenAI tool, the toolkit emphasizes safe scaling, ethical frameworks, and embedding communication best practices. It accompanies the publication The People Factor to promote responsible AI use across public sector organizations.

- [SAFE-AI A Framework for Securing AI-Enabled Systems](https://www.linkedin.com/feed/update/urn:li:activity:7347734136868020224) - Systems enabled with Artificial Intelligence technology demand special security considerations. A significant concern is the presence of supply chain vulnerabilities and the associated risks stemming from unclear provenance of AI models. Also, AI contributes to the attack surface through its inherent dependency on data and corresponding learning processes. Attacks include adversarial inputs, poisoning, exploiting automated decision-making, exploiting model biases, and exposure of sensitive information.

- [The General-Purpose AI Code of Practice – Safety & Security](https://www.linkedin.com/feed/update/urn:li:activity:7349138123362091009) — A voluntary EU framework designed to help providers of general-purpose AI models meet AI Act obligations on safety, transparency, and copyright. The Safety & Security chapter focuses on managing systemic risks in advanced models, outlining state-of-the-art practices for risk mitigation. Developed through a multi-stakeholder expert process, the code provides practical guidance and a path for legal compliance under Articles 53 and 55 of the AI Act.

## June 2025

- [Confidential AI Inference Systems](https://www.linkedin.com/feed/update/urn:li:activity:7341501922383695872) — Anthropic and Pattern Labs are exploring confidential inference—an approach for running AI models on sensitive data without exposing it to infrastructure operators or cloud providers. In a typical AI deployment, three parties are involved: the model owner, the user providing the data, and the cloud provider hosting the service. Without safeguards, each must trust the others with sensitive assets. Confidential inference eliminates this need by enforcing cryptographic boundaries—ensuring that neither the data nor the model is accessible outside the secure enclave, not even to the infrastructure host.

- [AI Red-Team Playbook for Security Leaders](https://www.linkedin.com/feed/update/urn:li:activity:7339375352231710721) — Hacken, Blockchain Security & Compliance’s playbook offers a strategic framework for safeguarding LLM systems through lifecycle-based adversarial testing. It identifies emerging risks—prompt injections, jailbreaks, RAG exploits, and data poisoning—while emphasizing real-time mitigation and multidisciplinary collaboration. It integrates methodologies like PASTA and STRIDE, aligning AI security with enterprise risk governance.

- [AI Security Market Report](https://www.linkedin.com/feed/update/urn:li:activity:7325564352156119040) — Security practitioners have been searching for a resource that clearly describes both what AI security challenges exist, and what solutions the market has provided. This report highlights the challenges and clearly states the maturity of various vendor offerings.

- [Fundamentals of Secure AI Systems with Personal Data](https://www.linkedin.com/feed/update/urn:li:activity:7337637796007817216) — is a training for cybersecurity professionals, developers and deployers of AI systems on AI security & Personal Data Protection addressing the current AI needs and skill gaps.

- [Security Risks in Artificial Intelligence for Finance](https://www.linkedin.com/feed/update/urn:li:activity:7341851628955758592) — Set of best practices intended for the Board and C-Level.

- [Disrupting malicious uses of AI: June 2025](https://www.linkedin.com/feed/update/urn:li:activity:7336790322426912768) — OpenAI continues its work to detect and prevent the misuse of AI, including threats like social engineering, cyber espionage, scams, and covert influence operations. In the last three months, AI tools have helped teams uncover and disrupt malicious campaigns, aligned with a broader mission to ensure AI is used safely and democratically.

- [Agentic AI Red Teaming Guide](https://www.linkedin.com/feed/update/urn:li:activity:7333874110684348417) — Agentic systems introduce new risks—autonomous reasoning, tool use, and multi-agent complexity—that traditional red teaming can’t fully address. This guide aims to fill that gap with practical, actionable steps.

- [AI Data Security — Best Practices for Securing Data Used to Train & Operate AI Systems](https://www.linkedin.com/feed/update/urn:li:activity:7331359099193872387) — This guidance highlights the critical role of data security in ensuring the accuracy, integrity, and trustworthiness of AI outcomes. It outlines key risks that may arise from data security and integrity issues across all phases of the AI lifecycle, from development and testing to deployment and operation.

## May 2025

- [AI Agent Governance: A Field Guide](https://www.linkedin.com/feed/update/urn:li:activity:7328876049524543489) — A deep dive into agent deployment risks and governance gaps, mapping scenarios, and introducing frameworks for oversight.

- [Agent Name Service (ANS) for Secure AI Agent Discovery](https://www.linkedin.com/feed/update/urn:li:activity:7328823746142568449) — Developed under the OWASP GenAI Security Project – Agentic Security Initiative, introduces a secure, DNS-inspired framework for AI agent discovery. ANS leverages Public Key Infrastructure (PKI) for identity verification, structured JSON schemas for communication, and a protocol adapter layer supporting A2A, MCP, and ACP protocols.

- [IDC: AI in Security Ops](https://www.linkedin.com/feed/update/urn:li:activity:7325564352156119040) — A survey of 900+ professionals highlighting AI use cases, deployment frictions, and the need for governance frameworks.

- [Securing AI: Addressing the OWASP Top 10 for Large Language Model Applications](https://www.icitech.org/post/securing-aiaddressing-the-owasp-top-10-forlarge-language-model-applications)

- [Cobalt & Cyentia Institute: State of Pentesting 2025](https://www.linkedin.com/feed/update/urn:li:activity:7324377543984189440) — Highlights AI-specific penetration testing trends, fix rates, and vulnerabilities in LLMs.

- [Check Point Software: State of AI Cybersecurity 2025](https://www.linkedin.com/feed/update/urn:li:activity:7323441087509839872) — A detailed look at GenAI use in phishing, malware, voice deepfakes, and criminal toolkits.

- [Cisco: State of AI Security](https://www.linkedin.com/feed/update/urn:li:activity:7319411865632010241) — Covers infrastructure threats, AI policy trends, and attack vectors like prompt injection.

- [Resemble AI: Deepfake Threats Q1 2025](https://www.resemble.ai/q1-2025-ai-deepfake-security-report/) — Quantifies deepfake-driven fraud, identity risks, and emerging multimodal threats.

- [IBM Security X-Force: GenAI Threat Highlights](https://www.linkedin.com/feed/update/urn:li:activity:7319100078298648576) — Breaks down GenAI misuse trends and the gap between adoption and risk ownership.

- [Imperva: Bad Bot Report](https://www.linkedin.com/feed/update/urn:li:activity:7318788439192006657) — Explores AI bot dominance in web traffic and major sources of automation.

- [ETSI SAI Baseline Security for AI Systems](https://www.etsi.org/deliver/etsi_ts/104200_104299/104223/01.01.01_60/ts_104223v010101p.pdf) — Introduces baseline cybersecurity requirements and verification guidance for AI systems.

- [Automating Deception: AI in Romance Fraud (CSEST)](https://www.linkedin.com/feed/update/urn:li:activity:7318016094340857856) — Explores the role of GenAI in industrializing romance fraud at scale.

- [AI Privacy Risks in LLMs — European Data Protection Board](https://www.linkedin.com/feed/update/urn:li:activity:7318291536704598016) — Analyzes risks like data leakage, prompt injection, and weak access controls across the LLM lifecycle.

- [Microsoft: Taxonomy of Failure Modes in Agentic AI](https://www.linkedin.com/feed/update/urn:li:activity:7323553744065900547) — Categorizes failures in agent-based systems, backed by internal red teaming and interviews, including a memory corruption case study.


