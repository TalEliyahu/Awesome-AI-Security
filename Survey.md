### Surveys 

#### LLM Backdoors
- [2025/02] **A Survey on Backdoor Threats in Large Language Models (LLMs): Attacks, Defenses, and Evaluations** — [arXiv](https://arxiv.org/abs/2502.05224)

#### Red Teaming
- [2025/03] **Building Safe GenAI Applications: An End-to-End Overview of Red Teaming for Large Language Models** — [arXiv](https://arxiv.org/abs/2503.01742)
- [2025/04] **Against The Achilles’ Heel: A Survey on Red Teaming for Generative Models** — [JAIR](https://www.jair.org/index.php/jair/article/view/17654)

#### LLM Security
- [2025/05] **LLM Security: Vulnerabilities, Attacks, Defenses, and Countermeasures** — [arXiv](https://arxiv.org/abs/2505.01177)

#### Attacks/Defenses
- [2025/05] **A Survey of Attacks on Large Language Models** — [arXiv](https://arxiv.org/abs/2505.12567)

#### Threats & Misuse
- [2025/05] **Security Concerns for Large Language Models: A Survey** — [arXiv](https://arxiv.org/abs/2505.18889)

#### Data/Model Poisoning
- [2025/06] **A Systematic Review of Poisoning Attacks Against Large Language Models** — [arXiv](https://arxiv.org/abs/2506.06518)

#### Model Extraction
- [2025/06] **A Survey on Model Extraction Attacks and Defenses for Large Language Models** — [arXiv](https://arxiv.org/abs/2506.22521)

#### Agent Threat Model
- [2025/06] **From Prompt Injections to Protocol Exploits: Threats in LLM-Powered AI Agents Workflows** — [arXiv](https://arxiv.org/abs/2506.23260)

#### Data Security
- [2025/08] **A Survey on Data Security in Large Language Models** — [arXiv](https://arxiv.org/abs/2508.02312)

#### Prompt Injection Threat Model
- [2025/09] **A Threat Model of Prompt-Based Attacks for Securing LLMs** — [arXiv](https://arxiv.org/abs/2509.04615)

#### Prompt Injection (Multimodal)

#### Cybersecurity Survey
- [2025/09] **Large Language Models in Cybersecurity: A Survey of Applications, Vulnerabilities, and Defense Techniques** — [MDPI](https://www.mdpi.com/2673-2688/6/9/216)
